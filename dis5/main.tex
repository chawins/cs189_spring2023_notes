\documentclass[letterpaper,12pt]{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2023} with \usepackage[nohyperref]{icml2023} above.
% \usepackage{hyperref}
\usepackage{natbib}
\usepackage[dvipsnames,svgnames,table]{xcolor}
\usepackage{multicol}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage{arxiv}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\input{math_commands.tex}

% if you use cleveref..

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
\usepackage[disable,textsize=tiny]{todonotes}
% \usepackage[textsize=tiny]{todonotes}

\usepackage{amsfonts}
\usepackage[utf8]{inputenc}
\usepackage{multirow}
\usepackage{makecell}
\usepackage[pagebackref]{hyperref}
\hypersetup{
    colorlinks=true,
    allcolors=DarkBlue
%   allbordercolors=red
}
\usepackage{courier}
\usepackage{xspace}
\usepackage{bm}
\usepackage{wrapfig}
\usepackage{oubraces}
\usepackage{amsthm}
\usepackage[linesnumbered,lined,boxed,commentsnumbered]{algorithm2e}
\usepackage{algorithmic}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{stfloats}
\usepackage{url}
\usepackage{enumitem}
\usepackage[capitalize,noabbrev]{cleveref}

\RestyleAlgo{ruled}
\definecolor{Gray}{gray}{0.7}
\newcolumntype{g}{>{\columncolor{Gray}}r}

\def\Ex{\mathop{\mathbb{E}}}
\def\P{\mathop{\mathrm{P}}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\sgn}[1]{\text{sign}\left(#1\right)}
\newcommand{\inner}[1]{\left\langle#1\right\rangle}
\def\minop{\mathop{\rm min}\limits}
\def\maxop{\mathop{\rm max}\limits}
\newcommand{\ber}[1]{\mathrm{Bern}\left(#1\right)}
\def\unif{\mathcal{U}}
\def\eqref#1{Eqn.~(\ref{#1})}
\def\figref#1{Fig.~\ref{#1}}

% \crefname{algocf}{alg.}{algs.}
\Crefname{algocf}{Algorithm}{Algorithms}

\newcommand{\chawin}[1]{\textcolor{green}{Chawin: #1}}
% \newcommand{\chawin}[1]{}
\newcommand{\note}[1]{\textcolor{blue}{Note: #1}}
% \newcommand{\todo}[1]{\textcolor{red}{TODO: #1}}
\newcommand{\question}[1]{\textcolor{orange}{\textit{#1}}}


\renewcommand{\algorithmiccomment}[1]{{\color{gray}\small \# \emph{#1}}}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
% \icmltitlerunning{Realistic Decision-Based Attacks on Machine Learning Systems\hfill\thepage}

\hypersetup{
pdftitle={CS189/289A Spring 2023},
pdfsubject={cs.CR,cs.CV,cs.LG},
pdfauthor={Chawin Sitawarin},
pdfkeywords={},
}


\title{CS189/289A Spring 2023: Linear Regression}

\author{
    Chawin Sitawarin \thanks{Corresponding email: \texttt{chawins@berkeley.edu}}
}

\date{
    University of California, Berkeley
}

\begin{document}

\maketitle

Demo: \url{https://colab.research.google.com/drive/1Pv1BUScUd6fHm2s2pQ9oLPHwg60QSYi8}

\section{Optimization View of Linear Regression}

One very nice thing about linear regression is that it can be tackled or studied from various perspectives and domains.
%
One can simply look at linear regression as a standalone optimization problem.
%
Specifically, given some matrix $\mX \in \R^{n \times d}$ and vector $\vy \in \R^n$, we wish to find the parameters $\vw \in \R^d$ that minimize the loss function $loss(w; \mX, \vy)$ which is chosen to be the mean (or sum) of squared errors (MSE):
\begin{align}
    \vw^*                               & = \argmin_{\vw \in \R^d}~ loss(\vw; \mX, \vy)                                                            \\
    \text{where} \quad L(\vw; \mX, \vy) & = \frac{1}{n} \sum_{i=1}^n \left( y_i - \vx_i^\top \vw \right)^2 = \frac{1}{n} \norm{\vy - \mX \vw}_2^2.
\end{align}
A simple interpretation is that we want to best predict or ``reconstruct'' any variable $y_i$ given only its feature vector $\vx_i$ by making an assumption that their relationship is linear.

\section{Probabilistic View of Linear Regression}

A different way to motivate linear regression is as a probabilistic model for the data.
%
Here, we assume that the data is generated by a linear model with additive Gaussian noise.
%
Or more specifically, we assume that the data is generated by the following model:
\begin{align}
    p(y \mid \vx, \vw) & = \mathcal{N}(y \mid \vx^\top \vw, \sigma^2), \label{eq:linearRegressionModel}
\end{align}
or in other words,
\begin{align}
    y & = \vx^\top \vw + \epsilon \qquad \text{where} \quad \epsilon \sim \mathcal{N}(0, \sigma^2)
\end{align}

\begin{itemize}[leftmargin=*]
    \item This view shows another example of MLE.
    \item This view will be useful when we study extensions of linear regression such as ridge regression and LASSO, particularly as a solution of the Maximum A Posteriori (MAP) problem.
\end{itemize}
%
In this setup, we can view linear regression as a way to estimate the parameters $\vw$ and $\sigma^2$ from the data via \emph{Maximum Likelihood Estimation} (MLE).

We write the Likelihood function as
\begin{align}
    L(\vw, \sigma \mid \mX, \vy) & = \prod_{i=1}^n p(y_i \mid \vx_i, \vw, \sigma^2) \label{eq:likelihood}                                                          \\
                                 & = \prod_{i=1}^n \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left( -\frac{1}{2 \sigma^2} \left( y_i - \vx_i^\top \vw \right)^2 \right) \\
                                 & = \left(2 \pi \sigma^2\right)^{-n/2} \exp \left( -\frac{1}{2 \sigma^2} \norm{\vy - \mX \vw}_2^2 \right)
\end{align}
We can miximize the likelihood directly, but more often than not, it is more convenient to minimize the \emph{Negative Log-Likelihood} (NLL) instead.
%
This term is frequently used in machine learning as well as most of the deep learning models.
\begin{align}
    \argmax_{\vw, \sigma}~ L(\vw, \sigma \mid \mX, \vy) & = \argmax_{\vw, \sigma}~ \log L(\vw, \sigma \mid \mX, \vy)                                                &  & \text{(Log-Likelihood)}          \\
                                                        & = - \argmin_{\vw, \sigma}~ \log L(\vw, \sigma \mid \mX, \vy)                                              &  & \text{(Negative Log-Likelihood)} \\
                                                        & = \argmin_{\vw, \sigma}~ \frac{1}{2 \sigma^2} \norm{\vy - \mX \vw}_2^2 + \frac{n}{2} \log (2\pi \sigma^2)
\end{align}

Most of the times, we focus more on finding $\vw^*$ and ignore $\sigma^*$.
%
To find $\vw^*$ we set the gradient to zero, i.e.,
\begin{align}
    \nabla_w L(\vw^*, \sigma \mid \mX, \vy)                                                                                         & = 0                                             \\
    \frac{\partial}{\partial \vw} \left( \frac{1}{2 \sigma^2} \norm{\vy - \mX \vw^*}_2^2 + \frac{n}{2} \log (2\pi \sigma^2) \right) & = 0 \label{eq:MLEvw}                            \\
    \frac{1}{\sigma^2} \mX^\top (\mX \vw^* - \vy)                                                                                   & = 0 \label{eq:MLEvw2}                           \\
    \mX^\top \vy - \mX^\top \mX \vw^*                                                                                               & = 0 \label{eq:MLEvw3}                           \\
    \mX^\top \mX \vw^*                                                                                                              & = \mX^\top \vy                                  \\
    \vw^*                                                                                                                           & = \left( \mX^\top \mX \right)^{-1} \mX^\top \vy
\end{align}

We call the final MSE produced by $\vw$ \emph{Residual Sum of Squares} (RSS).
\begin{align}
    RSS(\vw) = \norm{\vy - \hat{\vy}}_2^2 = \norm{\vy - \mX \vw}_2^2
\end{align}

\subsection*{Exercise 1}

Show that the MLE solution for $\sigma$ (i.e., $\sigma^*$) is given by $\sigma^* = \frac{1}{n} \norm{\vy - \mX \vw^*}_2^2$.

\subsection{Generative Model View of Linear Regression}

Instead of viewing linear regression as a way to estimate $p(y \mid \vx)$ using the parameters $\vw$ and $\sigma^2$, we can also view it as a ``generative model'' for the joint distribution $p(y, \vx)$ instead.
%
In this case, we make an assumption that $p(y, \vx)$ is \emph{jointly Guassian}, or precisely, we assume that they are generated from the following distribution:
\begin{align}
    p(y, \vx) & = \mathcal{N} \left( \begin{bmatrix} y \\ \vx \end{bmatrix} \mid \begin{bmatrix} \mu_y \\ \vmu_x \end{bmatrix}, \begin{bmatrix} \Sigma_{yy} & \mSigma_{xy} \\ \mSigma_{xy} & \mSigma_{xx} \end{bmatrix} \right)
\end{align}

Then we will estimate all the parameters $\mu_y$, $\vmu_x$, $\Sigma_{yy}$, $\mSigma_{xy}$, $\mSigma_{xx}$ from the data with MLE.
%
Given that the joint distribution is Gaussian, we know that the marginal distribution for each variable is also Gaussian.
%
As a result, we know that the following empirical means and covariances are the MLE solutions:
\begin{align}
    \hat{\mu}_y        & = \frac{1}{n} \sum_{i=1}^n y_i                                                                          \\
    \hat{\vmu}_x       & = \frac{1}{n} \sum_{i=1}^n \vx_i                                                                        \\
    \hat{\Sigma}_{yy}  & = \frac{1}{n} \sum_{i=1}^n \left( y_i - \hat{\mu}_y \right)^2                                           \\
    \hat{\mSigma}_{xy} & = \frac{1}{n} \sum_{i=1}^n \left( \vx_i - \hat{\vmu}_x \right) \left( y_i - \hat{\mu}_y \right)^\top    \\
    \hat{\mSigma}_{xx} & = \frac{1}{n} \sum_{i=1}^n \left( \vx_i - \hat{\vmu}_x \right) \left( \vx_i - \hat{\vmu}_x \right)^\top
\end{align}

Now we will show that $\E[y \mid x]$ takes the same form as the predicted value $\hat{y}$ in the previous section.
\begin{align}
    \E[y \mid x] & = \mu_{y \mid x}                                           \\
                 & = \mu_y + \mSigma_{xy}^\T \mSigma_{xx}^{-1} (\vx - \vmu_x)
\end{align}
Then plugging in the MLE solutions, we get:
\begin{align}
    \hat{y} \coloneqq f(\vx) & = \hat{\mu}_y + \hat{\mSigma}_{xy}^\T \hat{\mSigma}_{xx}^{-1} (\vx - \hat{\vmu}_x)                                                                                                                   \\
                             & = \hat{\mSigma}_{xy}^\T \hat{\mSigma}_{xx}^{-1} \vx + \left( \hat{\mu}_y  - \hat{\mSigma}_{xy}^\T \hat{\mSigma}_{xx}^{-1} \hat{\vmu}_x \right)                                                       \\
                             & = \vw_1^\T \vx + w_0                                                                                                                                                                                 \\
                             & = \begin{bmatrix} \hat{\mSigma}_{xy}^\T \hat{\mSigma}_{xx}^{-1} \\ \hat{\mu}_y  - \hat{\mSigma}_{xy}^\T \hat{\mSigma}_{xx}^{-1} \hat{\vmu}_x \end{bmatrix}^\T \begin{bmatrix} \vx \\ 1 \end{bmatrix}
\end{align}
It takes some algebra to convince that $[\vw_1; w_0] = \vw = \left( \mX^\top \mX \right)^{-1} \mX^\top \vy$ in the previous section if the rows of $\mX$ are $[\vx_i ~1]$'s (appended by $1$).


\section*{Acknowledgement}

This note is, for the most parts, based on \citet{pml1Book}.

\bibliography{reference.bib}
\bibliographystyle{abbrvnat}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
