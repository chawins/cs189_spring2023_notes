\documentclass[letterpaper,12pt]{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2023} with \usepackage[nohyperref]{icml2023} above.
% \usepackage{hyperref}
\usepackage{natbib}
\usepackage[dvipsnames,svgnames,table]{xcolor}
\usepackage{multicol}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage{arxiv}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\input{math_commands.tex}

% if you use cleveref..

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
\usepackage[disable,textsize=tiny]{todonotes}
% \usepackage[textsize=tiny]{todonotes}

\usepackage{amsfonts}
\usepackage[utf8]{inputenc}
\usepackage{multirow}
\usepackage{makecell}
\usepackage[pagebackref]{hyperref}
\hypersetup{
    colorlinks=true,
    allcolors=DarkBlue
%   allbordercolors=red
}
\usepackage{courier}
\usepackage{xspace}
\usepackage{bm}
\usepackage{wrapfig}
\usepackage{oubraces}
\usepackage{amsthm}
\usepackage[linesnumbered,lined,boxed,commentsnumbered]{algorithm2e}
\usepackage{algorithmic}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{stfloats}
\usepackage{url}
\usepackage{enumitem}
\usepackage[capitalize,noabbrev]{cleveref}

\RestyleAlgo{ruled}
\definecolor{Gray}{gray}{0.7}
\newcolumntype{g}{>{\columncolor{Gray}}r}

\def\Ex{\mathop{\mathbb{E}}}
\def\P{\mathop{\mathrm{P}}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\sgn}[1]{\text{sign}\left(#1\right)}
\newcommand{\inner}[1]{\left\langle#1\right\rangle}
\def\minop{\mathop{\rm min}\limits}
\def\maxop{\mathop{\rm max}\limits}
\newcommand{\ber}[1]{\mathrm{Bern}\left(#1\right)}
\def\unif{\mathcal{U}}
\def\eqref#1{Eqn.~(\ref{#1})}
\def\figref#1{Fig.~\ref{#1}}

% \crefname{algocf}{alg.}{algs.}
\Crefname{algocf}{Algorithm}{Algorithms}

\newcommand{\chawin}[1]{\textcolor{green}{Chawin: #1}}
% \newcommand{\chawin}[1]{}
\newcommand{\note}[1]{\textcolor{blue}{Note: #1}}
% \newcommand{\todo}[1]{\textcolor{red}{TODO: #1}}
\newcommand{\question}[1]{\textcolor{orange}{\textit{#1}}}


\renewcommand{\algorithmiccomment}[1]{{\color{gray}\small \# \emph{#1}}}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
% \icmltitlerunning{Realistic Decision-Based Attacks on Machine Learning Systems\hfill\thepage}

\hypersetup{
pdftitle={CS189 (Spring 2023) Notes},
pdfsubject={cs.CR,cs.CV,cs.LG},
pdfauthor={Chawin Sitawarin},
pdfkeywords={},
}


\title{CS189/289A Spring 2023\\Linear Discriminant Analysis}

\author{
    Chawin Sitawarin \thanks{Corresponding email: \texttt{chawins@berkeley.edu}}
}

\date{
    University of California, Berkeley
}

\begin{document}

\maketitle

Demo: \url{https://colab.research.google.com/drive/1jPgvRdsgHa3hjeUcCA5-AobfQ8qJrk9C}

\section{Introduction and Logistic Example}

Start by deriving the LDA decision boundary in the 2D case. Then, we will discuss the following topics to get a better intuition.

\emph{Discriminant function} for class $k$ given an input $x$ is $\delta_k(x)$.
Think of it as some kind of scores for $x$, and we will use this score to predict a label of $x$.
\begin{align}
    \hat{y}(x) & = \argmax_{k \in \mathcal{Y}} \delta_k(x)
\end{align}

\begin{itemize}[leftmargin=*]
    \item $\delta_k(x)$ can take many forms, but we usually model it after $\P(\rmY = k \mid \rmX = x)$.
    \item When $\delta_k(x)$ is a linear function of $x$, the decision boundary is also linear. This is also true for any \emph{monotonic} function of $\delta_k(x)$.
    \item For example, if we take the sigmoid function of some linear function, the decision boundary is still linear.
\end{itemize}

For instance, consider the following posterior probabilities:
\begin{align}
    f(z)                       & = \frac{1}{1 - \exp(-z)}                                                \\
    \P(\rmY = 1 \mid \rmX = x) & = \frac{1}{1 + \exp(-(\beta_1^\T x + \beta_0))}                         \\
                               & = \frac{\exp(\beta_1^\T x + \beta_0)}{1 + \exp(\beta_1^\T x + \beta_0)} \\
    \P(\rmY = 0 \mid \rmX = x) & = \frac{1}{1 + \exp(\beta_1^\T x + \beta_0)}
\end{align}
\begin{itemize}[leftmargin=*]
    \item Note that $\P(\rmY = 0 \mid \rmX = x) + \P(\rmY = 1 \mid \rmX = x) = 1$ as expected for a binary classification problem.
    \item The monotonic function that turns $\P(\rmY = 1 \mid \rmX = x)$ to a linear function is an inverse of the sigmoid or logistic function, $\log(p/(1-p))$. Sometimes, this is called a \emph{logit function}, and its output is called \emph{logits}.
\end{itemize}

Assume that the 0-1 loss function is used, we have a simple decision rule:
\begin{align}
    \hat{y}(x) & = \begin{cases}
                       1 & \text{if } \P(\rmY = 1 \mid \rmX = x) \geq \P(\rmY = 0 \mid \rmX = x) \\
                       0 & \text{otherwise}
                   \end{cases}                                         \\
               & = \begin{cases}
                       1 & \text{if } f(x) \ge 0 \\
                       0 & \text{otherwise}
                   \end{cases}                                                                                         \\
    f(x)       & \coloneqq \log \left(\frac{\P(\rmY = 1 \mid \rmX = x)}{\P(\rmY = 0 \mid \rmX = x)}\right)  = \beta_1^\T x + \beta_0
\end{align}
We can now recover the linear decision boundary.

\section{LDA Assumptions}

\begin{enumerate}[leftmargin=*]
    \item Class-conditioned density function of $\rmX$ is Gaussian.
          \begin{align}
              \rmX \mid \rmY = k         & \sim \mathcal{N}(\mu_k, \Sigma_k)                                                                                                                      \\
              \P(\rmX = x \mid \rmY = k) & = \frac{1}{\sqrt{(2\pi)^d \left| \Sigma_k \right|}} \exp \left( -\frac{1}{2} \left( x - \mu_k \right)^T \Sigma_k^{-1} \left( x - \mu_k \right) \right)
          \end{align}
    \item The covariance matrices are the same for all classes.
          \begin{align}
              \Sigma_k & = \Sigma \quad \forall k
          \end{align}
\end{enumerate}

We want to get the posterior so we are going to use Bayes rule here:
\begin{align*}
    \P(\rmY = k \mid \rmX = x) & = \frac{\P(\rmX = x \mid \rmY = k) \P(\rmY = k)}{\P(\rmX = x)}                                 \\
                               & = \frac{\P(\rmX = x \mid \rmY = k) \pi_k}{\P(\rmX = x)}                                        \\
                               & = \frac{\P(\rmX = x \mid \rmY = k) \pi_k}{\sum_{k'} \P(\rmX = x \mid \rmY = k') \P(\rmY = k')}
\end{align*}
Notice that the denominator is the same for all $k$, so we can ``ignore'' it.
%
It will just be a constant number added to all discriminant functions equally, $C = \log \P(\rmX = x)$.

\begin{align}
    \delta_k(x) & \coloneqq \log \P(\rmY = k \mid \rmX = x)              \nonumber                                                                                                                                 \\
                & = \log \P(\rmX = x \mid \rmY = k) \pi_k    + C        \nonumber                                                                                                                                  \\
                & = \log \pi_k - \frac{1}{2} \log \left| \Sigma_k \right| - \frac{1}{2} \left( \mu_k \right)^T \Sigma_k^{-1} \mu_k + \left( \mu_k \right)^T \Sigma_k^{-1} x - \frac{1}{2} x^T \Sigma_k^{-1} x + C'
\end{align}
This discriminant is actually not linear in $x$! But we will use the second assumption to show that the decision boundary is linear.

Considering the binary-class problem, the decision boundary is given by $\{z \mid \delta_0(z) = \delta_1(z) \}$.
\begin{align}
    \delta_0(x) & =  \delta_1(x)                                                                                                                        \\
    0           & = \delta_1(x) - \delta_0(x)                                                                                                           \\
                & = \log \left(\frac{\P(\rmY = 1 \mid \rmX = x)}{\P(\rmY = 0 \mid \rmX = x)}\right)                                                     \\
    % & \log \pi_0 - \frac{1}{2} \log \left| \Sigma \right| - \frac{1}{2} \left( \mu_0 \right)^T \Sigma^{-1} \mu_0 + \left( \mu_0 \right)^T \Sigma^{-1} x - \frac{1}{2} x^T \Sigma^{-1} x + C'                \\
    % & -              \log \pi_1 - \frac{1}{2} \log \left| \Sigma \right| - \frac{1}{2} \left( \mu_1 \right)^T \Sigma^{-1} \mu_1 + \left( \mu_1 \right)^T \Sigma^{-1} x - \frac{1}{2} x^T \Sigma^{-1} x + C' \\
                & = \log \frac{\pi_1}{\pi_0} - \frac{1}{2} (\mu_1 - \mu_0)^T \Sigma^{-1} (\mu_1 - \mu_0) + \left( \mu_1 - \mu_0 \right)^T \Sigma^{-1} x
\end{align}

Next we will do some more analysis and intuition checks.

\section{What happens when we don't account for priors?}

\begin{enumerate}
    \item \textbf{Demo}: Show that the LDA decision boundaries are not invariant to the priors.
    \item A common mistake is forgetting the prior which will yield a wrong decision boundary when we have \emph{imbalance} classes, i.e., $\pi_0 \ne \pi_1$.
    \item \question{How much worse does the error become in this case in terms of $\pi_0$ and $\pi_1$ (as well as $\mu_0, \mu_1, \Sigma$), assuming that LDA assumptions are both true?}
    \item \textbf{Demo}: Compare to linear SVM for Gaussian and uniform data. SVM is not affected by the priors when the data is \textbf{uniformly distributed and are linearly separable}.
\end{enumerate}

\section{What happens when variance is high?}

\begin{enumerate}
    \item \textbf{Demo}: Show high variance in data leads to also high variance in the LDA decision boundaries.
\end{enumerate}

\section{What happens when the covariance matrices are different between classes?}

\section{What happens when the covariance matrices are anisotropic?}

LDA makes no assumption that the covariance matrix have to be isotropic, i.e., $\Sigma = \sigma^2 I_n$ for some $\sigma \in \R$.

\begin{enumerate}
    \item \textbf{Demo}: Show that the LDA decision boundary is not \emph{necessarily} perpendicular to the line connecting the two class centroids (means).
    \item This is not true the other way around, i.e., given that the boudnary decision is perpendicular to the line connecting the centroids, it is not necessary that the covariance matrix is isotropic. \question{Can you give an example?}
    \item You will be proving this in Problem 4 of the worksheet.
\end{enumerate}


\section{Next Steps}

\begin{enumerate}
    \item Work on problem 1 and 2 in the worksheet.
\end{enumerate}

\section*{References}

This note is, for the most parts, based on \citet{hastie01statisticallearning} and some on the course lecture note by Prof. Shewchuk (\url{https://people.eecs.berkeley.edu/~jrs/189/lec/07.pdf}).

\bibliography{reference.bib}
\bibliographystyle{abbrvnat}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
